{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# End-to-End ML Workflow with Observability\n",
        "\n",
        "This notebook demonstrates a complete machine learning workflow for customer churn detection using Snowflake's ML capabilities. The workflow includes:\n",
        "\n",
        "1. **Environment Preparation** - Set up the ML environment and dependencies\n",
        "2. **Initial Data Ingestion** - Load and process the first batch of data\n",
        "3. **First Model Training** - Train and deploy the initial model\n",
        "4. **Iterative Model Improvement** - Continuously add data and retrain models with observability\n",
        "\n",
        "## Key Features\n",
        "- Customer churn prediction using sales and feedback data\n",
        "- Sentiment analysis of customer feedback using Cortex AI\n",
        "- Feature engineering with Snowflake analytical functions\n",
        "- Model Registry and Feature Store integration\n",
        "- Model monitoring and drift detection\n",
        "- Automated retraining based on performance metrics\n",
        "- Full ML observability and lineage tracking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Environment Preparation\n",
        "\n",
        "Setting up the ML environment, importing necessary packages, and configuring Snowflake connections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Packages imported successfully\n",
            "‚úÖ Snowflake session established\n"
          ]
        }
      ],
      "source": [
        "# Import essential packages for ML workflow\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import logging\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "import json\n",
        "\n",
        "# Snowflake packages\n",
        "from snowflake.snowpark.context import get_active_session\n",
        "from snowflake.snowpark import functions as F\n",
        "from snowflake.snowpark import types as T\n",
        "from snowflake.snowpark.types import DecimalType, FloatType, IntegerType, DoubleType, LongType\n",
        "\n",
        "# Snowflake ML packages\n",
        "from snowflake.ml.feature_store import FeatureStore, FeatureView, Entity, CreationMode\n",
        "from snowflake.ml.registry import Registry\n",
        "from snowflake.ml.model import type_hints\n",
        "from snowflake.cortex import sentiment\n",
        "from snowflake.snowpark import Session\n",
        "\n",
        "\n",
        "# ML packages  \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import f1_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Configure logging\n",
        "logger = logging.getLogger(\"e2e-ml-workflow\")\n",
        "numeric_types = (DecimalType, FloatType, IntegerType, DoubleType, LongType)\n",
        "\n",
        "print(\"‚úÖ Packages imported successfully\")\n",
        "\n",
        "# Get active Snowflake session\n",
        "session = Session.builder.getOrCreate()\n",
        "print(\"‚úÖ Snowflake session established\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Environment configured - Schema: E2E_DEMO\n",
            "üìç Working in Database: \"CC_ML_JOBS\", Schema: \"E2E_DEMO\"\n"
          ]
        }
      ],
      "source": [
        "# Environment Configuration\n",
        "SCHEMA = 'E2E_DEMO'\n",
        "WAREHOUSE = 'COMPUTE_WH'  # Modify as needed\n",
        "CHURN_WINDOW = 30  # Days to define churn\n",
        "\n",
        "# Create and use dedicated schema\n",
        "session.sql(f'CREATE OR REPLACE SCHEMA {SCHEMA}').collect()\n",
        "session.sql(f'USE SCHEMA {SCHEMA}').collect()\n",
        "session.sql('CREATE OR REPLACE STAGE ML_STAGE').collect()\n",
        "\n",
        "print(f\"‚úÖ Environment configured - Schema: {SCHEMA}\")\n",
        "\n",
        "# Get current context\n",
        "db = session.get_current_database()\n",
        "sc = session.get_current_schema()\n",
        "print(f\"üìç Working in Database: {db}, Schema: {sc}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model Registry created: E2E_DEMO_MODEL_REGISTRY\n",
            "‚úÖ Feature Store created: E2E_DEMO_FEATURE_STORE\n"
          ]
        }
      ],
      "source": [
        "# Setup Feature Store and Model Registry\n",
        "mr_schema = f'{sc}_MODEL_REGISTRY'.replace('\"', '')\n",
        "fs_schema = f'{sc}_FEATURE_STORE'.replace('\"', '')\n",
        "\n",
        "# Clean up and create Model Registry\n",
        "session.sql(f'DROP SCHEMA IF EXISTS {mr_schema}').collect()\n",
        "session.sql(f'DROP SCHEMA IF EXISTS {fs_schema}').collect()\n",
        "\n",
        "# Create Model Registry\n",
        "cs = session.get_current_schema()\n",
        "session.sql(f'CREATE SCHEMA {mr_schema}').collect()\n",
        "mr = Registry(session=session, database_name=db, schema_name=mr_schema)\n",
        "session.sql(f'USE SCHEMA {cs}').collect()\n",
        "\n",
        "# Create Feature Store\n",
        "fs = FeatureStore(\n",
        "    session=session, \n",
        "    database=db, \n",
        "    name=fs_schema,\n",
        "    default_warehouse=WAREHOUSE, \n",
        "    creation_mode=CreationMode.CREATE_IF_NOT_EXIST\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Model Registry created: {mr_schema}\")\n",
        "print(f\"‚úÖ Feature Store created: {fs_schema}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Initial Data Ingestion\n",
        "\n",
        "Setting up data tables, staging areas, and ingesting the first batch of data for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Creating core data tables...\n",
            "‚úÖ Core data tables created successfully\n",
            "   - SALES\n",
            "   - CUSTOMERS\n",
            "   - FEEDBACK_RAW\n",
            "   - FEEDBACK_RAW_STREAM\n",
            "   - FEEDBACK_SENTIMENT\n"
          ]
        }
      ],
      "source": [
        "# Set the right database and schema as creating the model registry and feature store can change it\n",
        "session.sql(f'USE DATABASE {db}').collect()\n",
        "session.sql(f'USE SCHEMA {sc}').collect()\n",
        "\n",
        "\n",
        "# Create core data tables\n",
        "print(\"üîÑ Creating core data tables...\")\n",
        "\n",
        "# Create SALES table\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE SALES (\n",
        "    TRANSACTION_ID VARCHAR,\n",
        "    CUSTOMER_ID VARCHAR,\n",
        "    TRANSACTION_DATE DATE,\n",
        "    DISCOUNT_APPLIED BOOLEAN,\n",
        "    NUM_ITEMS NUMBER,\n",
        "    PAYMENT_METHOD VARCHAR, \n",
        "    TOTAL_AMOUNT FLOAT\n",
        ")\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create CUSTOMERS table\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE CUSTOMERS (\n",
        "    CUSTOMER_ID VARCHAR,\n",
        "    AGE BIGINT,\n",
        "    CUSTOMER_SEGMENT VARCHAR,\n",
        "    GENDER VARCHAR,\n",
        "    LOCATION VARCHAR,\n",
        "    SIGNUP_DATE DATE\n",
        ")\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create FEEDBACK_RAW table\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE FEEDBACK_RAW (\n",
        "    CHAT_DATE DATE,\n",
        "    COMMENT VARCHAR,\n",
        "    CUSTOMER_ID VARCHAR,\n",
        "    FEEDBACK_ID VARCHAR,\n",
        "    INTERNAL_ID BIGINT\n",
        ")\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create stream for processing feedback\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE STREAM FEEDBACK_RAW_STREAM \n",
        "    ON TABLE FEEDBACK_RAW\n",
        "    APPEND_ONLY = TRUE\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create table for processed sentiment\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE FEEDBACK_SENTIMENT (\n",
        "    FEEDBACK_ID VARCHAR,\n",
        "    CHAT_DATE DATE,\n",
        "    CUSTOMER_ID VARCHAR,\n",
        "    INTERNAL_ID BIGINT,\n",
        "    COMMENT VARCHAR,\n",
        "    SENTIMENT FLOAT\n",
        ")\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"‚úÖ Core data tables created successfully\")\n",
        "print(\"   - SALES\")\n",
        "print(\"   - CUSTOMERS\") \n",
        "print(\"   - FEEDBACK_RAW\")\n",
        "print(\"   - FEEDBACK_RAW_STREAM\")\n",
        "print(\"   - FEEDBACK_SENTIMENT\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Staging area and tracking tables created\n"
          ]
        }
      ],
      "source": [
        "# Setup staging area and data ingestion tracking\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE STAGE CSV\n",
        "DIRECTORY = (ENABLE = TRUE)\n",
        "URL = 's3://sfquickstarts/vhol_end_2_end_ml_with_observability/';\n",
        "\"\"\").collect()\n",
        "\n",
        "# Create tracking table for file ingestion\n",
        "session.sql(\"\"\"\n",
        "CREATE OR REPLACE TABLE FILES_INGESTED (\n",
        "    YEAR INT,\n",
        "    MONTH INT,\n",
        "    FILE_TYPE VARCHAR,\n",
        "    FILE_NAME VARCHAR,\n",
        "    STAGE_NAME VARCHAR,\n",
        "    INGESTED BOOLEAN\n",
        ")\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"‚úÖ Staging area and tracking tables created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data ingestion utilities defined\n"
          ]
        }
      ],
      "source": [
        "# Data ingestion utility functions\n",
        "import re\n",
        "from typing import List, Tuple\n",
        "\n",
        "def get_year_month_files(session, stage_name: str, file_prefix: str) -> List[Tuple[int, int, str]]:\n",
        "    \"\"\"Extract year and month information from files in staging area\"\"\"\n",
        "    list_files_query = f\"LIST @{stage_name}\"\n",
        "    files = session.sql(list_files_query).collect()\n",
        "    \n",
        "    file_names = [file[\"name\"].split(\"/\")[-1] for file in files]\n",
        "    file_pattern = re.compile(rf\"{re.escape(file_prefix)}_(\\d+)_(\\d+)\\.csv\")\n",
        "    \n",
        "    results = []\n",
        "    for file_name in file_names:\n",
        "        match = file_pattern.match(file_name)\n",
        "        if match:\n",
        "            year, month = int(match.group(1)), int(match.group(2))\n",
        "            results.append((year, month, file_name, stage_name))\n",
        "    \n",
        "    return sorted(results)\n",
        "\n",
        "def insert_file_tracking(table, db, sc, files):\n",
        "    \"\"\"Track files for ingestion\"\"\"\n",
        "    for file in files:\n",
        "        year, month, file_name, stage_name = file\n",
        "        sql_cmd = f\"\"\"\n",
        "            INSERT INTO {db}.{sc}.FILES_INGESTED\n",
        "            (YEAR, MONTH, FILE_TYPE, FILE_NAME, STAGE_NAME, INGESTED)\n",
        "            VALUES ('{year}', '{month}', '{table}', '{file_name}', '{stage_name}', False)\n",
        "        \"\"\"\n",
        "        session.sql(sql_cmd).collect()\n",
        "\n",
        "def load_into_table(session, table_name, file_name):\n",
        "    \"\"\"Load CSV file into Snowflake table\"\"\"\n",
        "    sql_cmd = f\"\"\" \n",
        "        COPY INTO {table_name}\n",
        "        FROM {file_name}  \n",
        "        FILE_FORMAT = (TYPE = 'CSV' FIELD_OPTIONALLY_ENCLOSED_BY='\"')  \n",
        "        ON_ERROR = 'ABORT_STATEMENT';      \n",
        "    \"\"\"\n",
        "    session.sql(sql_cmd).collect()\n",
        "\n",
        "print(\"‚úÖ Data ingestion utilities defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Found 8 sales files and 13 feedback files\n",
            "‚úÖ Customer data loaded\n"
          ]
        }
      ],
      "source": [
        "# Discover and register files for ingestion\n",
        "stage_name = \"PUBLIC.CSV\"\n",
        "\n",
        "# Get available files\n",
        "sales_files = get_year_month_files(session, stage_name, 'sales')\n",
        "feedback_files = get_year_month_files(session, stage_name, 'feedback_raw')\n",
        "\n",
        "# Track files for ingestion\n",
        "insert_file_tracking('sales', db, sc, sales_files)\n",
        "insert_file_tracking('feedback_raw', db, sc, feedback_files)\n",
        "\n",
        "print(f\"üìã Found {len(sales_files)} sales files and {len(feedback_files)} feedback files\")\n",
        "\n",
        "# Load customers (static data)\n",
        "load_into_table(session, f'{db}.{sc}.CUSTOMERS', f'@{stage_name}/customers.csv')\n",
        "print(\"‚úÖ Customer data loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data loading functions defined\n"
          ]
        }
      ],
      "source": [
        "# Load initial data (first 4 months for training)\n",
        "def copy_next_file(session, db: str, sc: str):\n",
        "    \"\"\"Copy the next unprocessed file from staging to tables\"\"\"\n",
        "    files_df = session.table(f'{db}.{sc}.FILES_INGESTED')\n",
        "    \n",
        "    # Get next sales file\n",
        "    sales_file = files_df.filter(\n",
        "        (F.col(\"file_type\") == 'sales') & (F.col(\"ingested\") == False)\n",
        "    ).select(\"year\", \"month\", \"file_name\", \"stage_name\").order_by(\"year\", \"month\").limit(1)\n",
        "    \n",
        "    sales_pd = sales_file.to_pandas()\n",
        "    if sales_pd.empty:\n",
        "        print(\"No unprocessed sales files found.\")\n",
        "        return False\n",
        "    \n",
        "    # Load sales data\n",
        "    year, month = int(sales_pd.YEAR[0]), int(sales_pd.MONTH[0])\n",
        "    file_name, stage_name = sales_pd.FILE_NAME[0], sales_pd.STAGE_NAME[0]\n",
        "    \n",
        "    load_into_table(session, f'{db}.{sc}.SALES', f'@{stage_name}/{file_name}')\n",
        "    \n",
        "    # Mark sales file as processed\n",
        "    session.sql(f\"\"\"\n",
        "        UPDATE {db}.{sc}.FILES_INGESTED\n",
        "        SET INGESTED = TRUE\n",
        "        WHERE FILE_NAME = '{file_name}' AND FILE_TYPE = 'sales'\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    # Load corresponding feedback file\n",
        "    feedback_file = files_df.filter(\n",
        "        (F.col(\"file_type\") == 'feedback_raw') & \n",
        "        (F.col(\"ingested\") == False) &\n",
        "        (F.col(\"YEAR\") == year) & \n",
        "        (F.col(\"MONTH\") == month)\n",
        "    ).limit(1)\n",
        "    \n",
        "    if feedback_file.count() > 0:\n",
        "        feedback_pd = feedback_file.to_pandas()\n",
        "        feedback_name = feedback_pd.FILE_NAME[0]\n",
        "        load_into_table(session, f'{db}.{sc}.FEEDBACK_RAW', f'@{stage_name}/{feedback_name}')\n",
        "        \n",
        "        # Mark feedback file as processed\n",
        "        session.sql(f\"\"\"\n",
        "            UPDATE {db}.{sc}.FILES_INGESTED\n",
        "            SET INGESTED = TRUE\n",
        "            WHERE FILE_NAME = '{feedback_name}' AND FILE_TYPE = 'feedback_raw'\n",
        "        \"\"\").collect()\n",
        "    \n",
        "    print(f\"üì• Loaded data for {year}-{month:02d}\")\n",
        "    return True\n",
        "\n",
        "# Sentiment processing function\n",
        "def process_sentiment():\n",
        "    \"\"\"Process sentiment for new feedback using Cortex AI\"\"\"\n",
        "    feedback_stream_df = session.table(\"FEEDBACK_RAW_STREAM\")\n",
        "    \n",
        "    if feedback_stream_df.count() > 0:\n",
        "        cols = ['FEEDBACK_ID', 'CHAT_DATE', 'CUSTOMER_ID', 'INTERNAL_ID', 'COMMENT']\n",
        "        feedback_sentiment_df = feedback_stream_df.select(cols).with_columns(\n",
        "            [\"SENTIMENT\"], [sentiment(F.col(\"COMMENT\"))]\n",
        "        )\n",
        "        feedback_sentiment_df.write.mode(\"append\").save_as_table(\"FEEDBACK_SENTIMENT\")\n",
        "        print(\"‚úÖ Sentiment processed for new feedback\")\n",
        "\n",
        "print(\"‚úÖ Data loading functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Feature engineering function defined\n"
          ]
        }
      ],
      "source": [
        "# Feature Engineering Functions\n",
        "def create_customer_features(session, db: str, sc: str, cur_date: datetime, table_name: str):\n",
        "    \"\"\"Create customer behavioral features for churn prediction\"\"\"\n",
        "    \n",
        "    # Load data tables\n",
        "    customers_df = session.table(f'{db}.{sc}.CUSTOMERS')\n",
        "    sales_df = session.table(f'{db}.{sc}.SALES').filter(F.col(\"TRANSACTION_DATE\") < F.lit(cur_date))\n",
        "    feedback_df = session.table(f'{db}.{sc}.FEEDBACK_SENTIMENT').filter(F.col(\"CHAT_DATE\") < F.lit(cur_date))\n",
        "    \n",
        "    # Sales aggregations by customer\n",
        "    sales_agg_df = sales_df.group_by(\"CUSTOMER_ID\").agg(\n",
        "        F.max(\"TRANSACTION_DATE\").alias(\"LAST_PURCHASE_DATE\"),\n",
        "        F.sum(\"TOTAL_AMOUNT\").alias(\"TOTAL_CUSTOMER_VALUE\")\n",
        "    )\n",
        "    \n",
        "    # Custom column naming for time-series features\n",
        "    def custom_column_naming(input_col, agg, window):\n",
        "        return f\"{agg}_{input_col}_{window.replace('-', 'PAST_')}\"\n",
        "    \n",
        "    # Time-series aggregations\n",
        "    sales_ts_df = sales_df.analytics.time_series_agg(\n",
        "        time_col=\"TRANSACTION_DATE\",\n",
        "        aggs={\"TOTAL_AMOUNT\": [\"SUM\", \"COUNT\"]},\n",
        "        windows=[\"-7D\", \"-1MM\", \"-2MM\", \"-3MM\"],\n",
        "        sliding_interval=\"1D\",\n",
        "        group_by=[\"CUSTOMER_ID\"],\n",
        "        col_formatter=custom_column_naming\n",
        "    )\n",
        "    \n",
        "    # Join sales aggregations\n",
        "    sales_features_df = sales_agg_df.join(\n",
        "        sales_ts_df,\n",
        "        (sales_agg_df.LAST_PURCHASE_DATE == sales_ts_df.TRANSACTION_DATE) &\n",
        "        (sales_agg_df.CUSTOMER_ID == sales_ts_df.CUSTOMER_ID),\n",
        "        \"left\"\n",
        "    ).select(\n",
        "        sales_agg_df[\"CUSTOMER_ID\"].alias(\"CUSTOMER_ID\"),\n",
        "        sales_agg_df[\"TOTAL_CUSTOMER_VALUE\"],\n",
        "        sales_agg_df[\"LAST_PURCHASE_DATE\"],\n",
        "        sales_ts_df[\"SUM_TOTAL_AMOUNT_PAST_7D\"],\n",
        "        sales_ts_df[\"SUM_TOTAL_AMOUNT_PAST_1MM\"],\n",
        "        sales_ts_df[\"SUM_TOTAL_AMOUNT_PAST_2MM\"],\n",
        "        sales_ts_df[\"SUM_TOTAL_AMOUNT_PAST_3MM\"],\n",
        "        sales_ts_df[\"COUNT_TOTAL_AMOUNT_PAST_7D\"].alias(\"COUNT_ORDERS_PAST_7D\"),\n",
        "        sales_ts_df[\"COUNT_TOTAL_AMOUNT_PAST_1MM\"].alias(\"COUNT_ORDERS_PAST_1MM\"),\n",
        "        sales_ts_df[\"COUNT_TOTAL_AMOUNT_PAST_2MM\"].alias(\"COUNT_ORDERS_PAST_2MM\"),\n",
        "        sales_ts_df[\"COUNT_TOTAL_AMOUNT_PAST_3MM\"].alias(\"COUNT_ORDERS_PAST_3MM\")\n",
        "    )\n",
        "    \n",
        "    # Feedback features\n",
        "    latest_feedback_df = feedback_df.group_by(\"CUSTOMER_ID\").agg(F.max(\"CHAT_DATE\").alias(\"CHAT_DATE\"))\n",
        "    \n",
        "    feedback_agg_df = feedback_df.analytics.moving_agg(\n",
        "        aggs={\"SENTIMENT\": [\"MIN\", \"AVG\"]},\n",
        "        window_sizes=[2, 3, 4],\n",
        "        order_by=[\"CHAT_DATE\"],\n",
        "        group_by=[\"CUSTOMER_ID\"]\n",
        "    )\n",
        "    \n",
        "    feedback_features_df = latest_feedback_df.join(feedback_agg_df, \"CUSTOMER_ID\", \"left\").select(\n",
        "        latest_feedback_df[\"CUSTOMER_ID\"],\n",
        "        feedback_agg_df[\"SENTIMENT_MIN_2\"],\n",
        "        feedback_agg_df[\"SENTIMENT_MIN_3\"],\n",
        "        feedback_agg_df[\"SENTIMENT_MIN_4\"],\n",
        "        feedback_agg_df[\"SENTIMENT_AVG_2\"],\n",
        "        feedback_agg_df[\"SENTIMENT_AVG_3\"],\n",
        "        feedback_agg_df[\"SENTIMENT_AVG_4\"]\n",
        "    )\n",
        "    \n",
        "    # Combine all features\n",
        "    features_df = customers_df.join(sales_features_df, \"CUSTOMER_ID\", \"left\") \\\n",
        "                             .join(feedback_features_df, \"CUSTOMER_ID\", \"left\") \\\n",
        "                             .select(\n",
        "                                 customers_df[\"CUSTOMER_ID\"],\n",
        "                                 customers_df[\"AGE\"],\n",
        "                                 customers_df[\"GENDER\"],\n",
        "                                 customers_df[\"LOCATION\"],\n",
        "                                 customers_df[\"CUSTOMER_SEGMENT\"],\n",
        "                                 sales_features_df[\"LAST_PURCHASE_DATE\"],\n",
        "                                 feedback_features_df[\"SENTIMENT_MIN_2\"],\n",
        "                                 feedback_features_df[\"SENTIMENT_MIN_3\"],\n",
        "                                 feedback_features_df[\"SENTIMENT_MIN_4\"],\n",
        "                                 feedback_features_df[\"SENTIMENT_AVG_2\"],\n",
        "                                 feedback_features_df[\"SENTIMENT_AVG_3\"],\n",
        "                                 feedback_features_df[\"SENTIMENT_AVG_4\"],\n",
        "                                 sales_features_df[\"SUM_TOTAL_AMOUNT_PAST_7D\"],\n",
        "                                 sales_features_df[\"SUM_TOTAL_AMOUNT_PAST_1MM\"],\n",
        "                                 sales_features_df[\"SUM_TOTAL_AMOUNT_PAST_2MM\"],\n",
        "                                 sales_features_df[\"SUM_TOTAL_AMOUNT_PAST_3MM\"],\n",
        "                                 sales_features_df[\"COUNT_ORDERS_PAST_7D\"],\n",
        "                                 sales_features_df[\"COUNT_ORDERS_PAST_1MM\"],\n",
        "                                 sales_features_df[\"COUNT_ORDERS_PAST_2MM\"],\n",
        "                                 sales_features_df[\"COUNT_ORDERS_PAST_3MM\"],\n",
        "                                 F.datediff(\"day\", sales_features_df[\"LAST_PURCHASE_DATE\"], F.lit(cur_date)).alias(\"DAYS_SINCE_LAST_PURCHASE\"),\n",
        "                                 F.lit(cur_date).alias(\"TIMESTAMP\")\n",
        "                             ).filter(sales_features_df[\"LAST_PURCHASE_DATE\"].isNotNull()) \\\n",
        "                              .dropDuplicates([\"CUSTOMER_ID\", \"TIMESTAMP\"])\n",
        "    \n",
        "    # Fill nulls with 0\n",
        "    fill_columns = [\n",
        "        \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \n",
        "        \"SENTIMENT_AVG_2\", \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n",
        "        \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \n",
        "        \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n",
        "        \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \n",
        "        \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n",
        "    ]\n",
        "    \n",
        "    for column in fill_columns:\n",
        "        features_df = features_df.fillna({column: 0})\n",
        "    \n",
        "    # Save features\n",
        "    features_df.write.mode(\"append\").save_as_table(table_name)\n",
        "    print(f\"‚úÖ Features created for {cur_date}\")\n",
        "\n",
        "print(\"‚úÖ Feature engineering function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Labeling function defined\n"
          ]
        }
      ],
      "source": [
        "# Labeling function for churn detection\n",
        "def create_churn_labels(session, db: str, sc: str, features_table: str, output_table: str, churn_days: int):\n",
        "    \"\"\"Label customers as churned based on future purchase behavior\"\"\"\n",
        "    \n",
        "    features_df = session.table(f'{db}.{sc}.{features_table}')\n",
        "    sales_df = session.table(f'{db}.{sc}.SALES')\n",
        "\n",
        "    sales_filtered = sales_df.select(F.col(\"CUSTOMER_ID\"), F.col(\"TRANSACTION_DATE\"))\n",
        "\n",
        "    # Find next transaction for each customer after their feature timestamp\n",
        "    next_transaction_df = features_df.join(\n",
        "        sales_filtered.select(\"CUSTOMER_ID\", \"TRANSACTION_DATE\"),\n",
        "        \"CUSTOMER_ID\",\n",
        "        \"left\"\n",
        "    ).filter(\n",
        "        F.col(\"TRANSACTION_DATE\") > F.col(\"LAST_PURCHASE_DATE\")\n",
        "    ).group_by(\"CUSTOMER_ID\", \"TIMESTAMP\").agg(\n",
        "        F.min(\"TRANSACTION_DATE\").alias(\"NEXT_TRANSACTION_DATE\")\n",
        "    )\n",
        "    \n",
        "    # Create labeled dataset\n",
        "    labeled_df = features_df.join(\n",
        "        next_transaction_df, \n",
        "        [\"CUSTOMER_ID\", \"TIMESTAMP\"], \n",
        "        \"left\"\n",
        "    ).select(\n",
        "        features_df[\"*\"],\n",
        "        F.when(\n",
        "            (F.col(\"NEXT_TRANSACTION_DATE\").is_null()) |\n",
        "            ((F.col(\"NEXT_TRANSACTION_DATE\") - F.col(\"LAST_PURCHASE_DATE\")) > churn_days),\n",
        "            1\n",
        "        ).otherwise(0).alias(\"CHURNED\"),\n",
        "        F.col(\"NEXT_TRANSACTION_DATE\")\n",
        "    )\n",
        "    \n",
        "    # Save labeled dataset\n",
        "    labeled_df.write.mode(\"overwrite\").save_as_table(output_table)\n",
        "    print(f\"‚úÖ Labels created for churn window: {churn_days} days\")\n",
        "\n",
        "print(\"‚úÖ Labeling function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Loading initial training data (4 months)...\n",
            "\n",
            "--- Loading month 1/4 ---\n",
            "üì• Loaded data for 2024-05\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "‚úÖ Features created for 2024-05-31\n",
            "‚úÖ Labels created for churn window: 30 days\n",
            "‚è±Ô∏è  Month 1 completed in 23.8s\n",
            "\n",
            "--- Loading month 2/4 ---\n",
            "üì• Loaded data for 2024-06\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "‚úÖ Features created for 2024-06-30\n",
            "‚úÖ Labels created for churn window: 30 days\n",
            "‚è±Ô∏è  Month 2 completed in 24.1s\n",
            "\n",
            "--- Loading month 3/4 ---\n",
            "üì• Loaded data for 2024-07\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "‚úÖ Features created for 2024-07-31\n",
            "‚úÖ Labels created for churn window: 30 days\n",
            "‚è±Ô∏è  Month 3 completed in 25.9s\n",
            "\n",
            "--- Loading month 4/4 ---\n",
            "üì• Loaded data for 2024-08\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "‚úÖ Features created for 2024-08-31\n",
            "‚úÖ Labels created for churn window: 30 days\n",
            "‚è±Ô∏è  Month 4 completed in 25.3s\n",
            "\n",
            "‚úÖ Initial data loading completed\n",
            "üìä Data Summary:\n",
            "   Sales records: 29,540\n",
            "   Customers: 5,000\n",
            "   Feedback records: 3,318\n",
            "\n",
            "üìä Label Distribution by Timestamp:\n",
            "   2024-05-31: 1790 not churned, 749 churned (29.5% churn rate)\n",
            "   2024-06-30: 1945 not churned, 2307 churned (54.3% churn rate)\n",
            "   2024-07-31: 2266 not churned, 2269 churned (50.0% churn rate)\n",
            "   2024-08-31: 283 not churned, 4253 churned (93.8% churn rate)\n",
            "‚úÖ Features and labels created\n"
          ]
        }
      ],
      "source": [
        "# Define table names\n",
        "features_table = 'CUSTOMER_FEATURES'\n",
        "labeled_table = 'CUSTOMER_FEATURES_LABELED'\n",
        "\n",
        "# Drop existing tables for fresh start\n",
        "session.sql(f'DROP TABLE IF EXISTS {features_table}').collect()\n",
        "session.sql(f'DROP TABLE IF EXISTS {labeled_table}').collect()\n",
        "\n",
        "# Create features for each month loaded\n",
        "sales_df = session.table(\"SALES\")\n",
        "\n",
        "# Load first 4 months of data for initial training\n",
        "print(\"üîÑ Loading initial training data (4 months)...\")\n",
        "\n",
        "for i in range(4):\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    print(f\"\\n--- Loading month {i+1}/4 ---\")\n",
        "    if not copy_next_file(session, db, sc):\n",
        "        break\n",
        "        \n",
        "    # Process sentiment for new feedback\n",
        "    process_sentiment()\n",
        "    \n",
        "    #calculate features for the latest transaction timestamp\n",
        "    latest_transaction = sales_df.select(F.max(F.col(\"transaction_date\"))).collect()[0][0]\n",
        "\n",
        "    create_customer_features(session, db, sc, latest_transaction, features_table)\n",
        "\n",
        "    #create churn labels\n",
        "    create_churn_labels(session, db, sc, features_table, labeled_table, CHURN_WINDOW)\n",
        "\n",
        "\n",
        "    elapsed = datetime.now() - start_time\n",
        "    print(f\"‚è±Ô∏è  Month {i+1} completed in {elapsed.total_seconds():.1f}s\")\n",
        "\n",
        "print(\"\\n‚úÖ Initial data loading completed\")\n",
        "\n",
        "# Check data loaded\n",
        "sales_count = session.table(\"SALES\").count()\n",
        "customers_count = session.table(\"CUSTOMERS\").count()\n",
        "feedback_count = session.table(\"FEEDBACK_SENTIMENT\").count()\n",
        "\n",
        "print(f\"üìä Data Summary:\")\n",
        "print(f\"   Sales records: {sales_count:,}\")\n",
        "print(f\"   Customers: {customers_count:,}\")\n",
        "print(f\"   Feedback records: {feedback_count:,}\")\n",
        "\n",
        "# Check label distribution\n",
        "label_distribution = session.sql(f\"\"\"\n",
        "    SELECT \n",
        "        TIMESTAMP,\n",
        "        SUM(CASE WHEN CHURNED = 0 THEN 1 ELSE 0 END) AS NOT_CHURNED,\n",
        "        SUM(CASE WHEN CHURNED = 1 THEN 1 ELSE 0 END) AS CHURNED\n",
        "    FROM {labeled_table}\n",
        "    GROUP BY TIMESTAMP\n",
        "    ORDER BY TIMESTAMP\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\\nüìä Label Distribution by Timestamp:\")\n",
        "for row in label_distribution:\n",
        "    timestamp = row[\"TIMESTAMP\"]\n",
        "    not_churned = row[\"NOT_CHURNED\"]\n",
        "    churned = row[\"CHURNED\"]\n",
        "    total = not_churned + churned\n",
        "    churn_rate = churned / total * 100 if total > 0 else 0\n",
        "    print(f\"   {timestamp}: {not_churned} not churned, {churned} churned ({churn_rate:.1f}% churn rate)\")\n",
        "\n",
        "print(\"‚úÖ Features and labels created\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. First Model Training\n",
        "\n",
        "Creating features, setting up the Feature Store, training the initial model, and registering it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Setting up Feature Store...\n",
            "‚úÖ Customer entity created\n",
            "‚úÖ Feature View FV_CUSTOMER_CHURN_V_1 created\n",
            "\n",
            "üìã Feature View registered in Feature Store:\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "|\"NAME\"             |\"VERSION\"  |\"DATABASE_NAME\"  |\"SCHEMA_NAME\"           |\"CREATED_ON\"                |\"OWNER\"        |\"DESC\"                                 |\"ENTITIES\"        |\"REFRESH_FREQ\"  |\"REFRESH_MODE\"  |\"SCHEDULING_STATE\"  |\"WAREHOUSE\"  |\"CLUSTER_BY\"  |\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "|FV_CUSTOMER_CHURN  |V_1        |CC_ML_JOBS       |E2E_DEMO_FEATURE_STORE  |2025-08-25 07:08:10.469000  |SPCS_PSE_ROLE  |Features for customer churn detection  |[                 |NULL            |NULL            |NULL                |NULL         |NULL          |\n",
            "|                   |           |                 |                        |                            |               |                                       |  \"CUSTOMER_ENT\"  |                |                |                    |             |              |\n",
            "|                   |           |                 |                        |                            |               |                                       |]                 |                |                |                    |             |              |\n",
            "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Setup Feature Store\n",
        "print(\"üîÑ Setting up Feature Store...\")\n",
        "\n",
        "# Create entity for customers\n",
        "if \"CUSTOMER_ENT\" not in json.loads(fs.list_entities().select(F.to_json(F.array_agg(\"NAME\", True))).collect()[0][0]):\n",
        "    customer_entity = Entity(\n",
        "        name=\"CUSTOMER_ENT\", \n",
        "        join_keys=[\"CUSTOMER_ID\"],\n",
        "        desc=\"Primary Key for CUSTOMER\"\n",
        "    )\n",
        "    fs.register_entity(customer_entity)\n",
        "else:\n",
        "    customer_entity = fs.get_entity(\"CUSTOMER_ENT\")\n",
        "\n",
        "print(\"‚úÖ Customer entity created\")\n",
        "\n",
        "# Create Feature View\n",
        "labeled_df = session.table(f'{sc}.{labeled_table}')\n",
        "\n",
        "fv_name = \"FV_CUSTOMER_CHURN\"\n",
        "fv_version = \"V_1\"\n",
        "\n",
        "try:\n",
        "    feature_view = fs.get_feature_view(name=fv_name, version=fv_version)\n",
        "    print(f\"‚úÖ Feature View {fv_name}_{fv_version} already exists\")\n",
        "except:\n",
        "    feature_view_instance = FeatureView(\n",
        "        name=fv_name, \n",
        "        entities=[customer_entity], \n",
        "        feature_df=labeled_df,\n",
        "        timestamp_col=\"TIMESTAMP\",\n",
        "        refresh_freq=None,\n",
        "        desc=\"Features for customer churn detection\"\n",
        "    )\n",
        "    \n",
        "    feature_view = fs.register_feature_view(\n",
        "        feature_view=feature_view_instance, \n",
        "        version=fv_version, \n",
        "        block=True\n",
        "    )\n",
        "    print(f\"‚úÖ Feature View {fv_name}_{fv_version} created\")\n",
        "\n",
        "# Show feature view info\n",
        "print(\"\\nüìã Feature View registered in Feature Store:\")\n",
        "fs.list_feature_views().show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Creating datasets for training...\n",
            "Training timestamp: 2024-06-30\n",
            "Validation timestamp: 2024-07-31\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Training dataset: 4,252 records\n",
            "‚úÖ Validation dataset: 4,535 records\n"
          ]
        }
      ],
      "source": [
        "# Create training and validation datasets\n",
        "print(\"üîÑ Creating datasets for training...\")\n",
        "\n",
        "# Get available timestamps\n",
        "timestamps = session.table(labeled_table).select(\"TIMESTAMP\").distinct().sort(\"TIMESTAMP\").collect()\n",
        "\n",
        "# Use second timestamp for training, third for validation  \n",
        "training_timestamp = timestamps[1][\"TIMESTAMP\"]\n",
        "validation_timestamp = timestamps[2][\"TIMESTAMP\"]\n",
        "\n",
        "print(f\"Training timestamp: {training_timestamp}\")\n",
        "print(f\"Validation timestamp: {validation_timestamp}\")\n",
        "\n",
        "def create_dataset(fs, feature_view, name, timestamp):\n",
        "    \"\"\"Create dataset from Feature Store\"\"\"\n",
        "    spine_df = feature_view.feature_df.filter(\n",
        "        F.col(\"TIMESTAMP\") == F.lit(timestamp)\n",
        "    ).group_by('CUSTOMER_ID').agg(F.max('TIMESTAMP').as_('TIMESTAMP'))\n",
        "    \n",
        "    dataset = fs.generate_dataset(\n",
        "        name=name, \n",
        "        version='v1',\n",
        "        spine_df=spine_df, \n",
        "        features=[feature_view], \n",
        "        spine_timestamp_col='TIMESTAMP'\n",
        "    )\n",
        "    \n",
        "    # Convert to Snowpark DataFrame and handle data types\n",
        "    dataset_df = dataset.read.to_snowpark_dataframe()\n",
        "    \n",
        "    # Convert decimal columns to float\n",
        "    decimal_columns = [field.name for field in dataset_df.schema.fields\n",
        "                      if isinstance(field.datatype, numeric_types)]\n",
        "    \n",
        "    for column_name in decimal_columns:\n",
        "        dataset_df = dataset_df.with_column(\n",
        "            column_name,\n",
        "            F.col(column_name).cast(\"float\")\n",
        "        )\n",
        "    \n",
        "    return dataset_df\n",
        "\n",
        "# Create datasets\n",
        "training_dataset = create_dataset(fs, feature_view, 'CHURN_TRAINING', training_timestamp)\n",
        "validation_dataset = create_dataset(fs, feature_view, 'CHURN_VALIDATION', validation_timestamp)\n",
        "\n",
        "print(f\"‚úÖ Training dataset: {training_dataset.count():,} records\")\n",
        "print(f\"‚úÖ Validation dataset: {validation_dataset.count():,} records\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Training initial churn prediction model...\n",
            "‚úÖ Model training completed:\n",
            "   Training F1 Score: 0.9595\n",
            "   Test F1 Score: 0.6667\n",
            "   Validation F1 Score: 0.7670\n"
          ]
        }
      ],
      "source": [
        "# Train initial model\n",
        "print(\"üîÑ Training initial churn prediction model...\")\n",
        "\n",
        "# Define feature columns\n",
        "categorical_cols = ['GENDER', 'LOCATION', 'CUSTOMER_SEGMENT']\n",
        "numerical_cols = [\n",
        "    \"AGE\", \"SENTIMENT_MIN_2\", \"SENTIMENT_MIN_3\", \"SENTIMENT_MIN_4\", \n",
        "    \"SENTIMENT_AVG_2\", \"SENTIMENT_AVG_3\", \"SENTIMENT_AVG_4\",\n",
        "    \"SUM_TOTAL_AMOUNT_PAST_7D\", \"SUM_TOTAL_AMOUNT_PAST_1MM\", \n",
        "    \"SUM_TOTAL_AMOUNT_PAST_2MM\", \"SUM_TOTAL_AMOUNT_PAST_3MM\",\n",
        "    \"COUNT_ORDERS_PAST_7D\", \"COUNT_ORDERS_PAST_1MM\", \n",
        "    \"COUNT_ORDERS_PAST_2MM\", \"COUNT_ORDERS_PAST_3MM\"\n",
        "]\n",
        "feature_cols = categorical_cols + numerical_cols\n",
        "target_col = \"CHURNED\"\n",
        "\n",
        "def train_churn_model(feature_df):\n",
        "    \"\"\"Train XGBoost model for churn prediction\"\"\"\n",
        "    \n",
        "    # Convert to pandas\n",
        "    train_df = feature_df.to_pandas()\n",
        "    \n",
        "    # Split data\n",
        "    train_data, test_data = train_test_split(train_df, test_size=0.2, random_state=111)\n",
        "    \n",
        "    # Create preprocessing pipeline\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"ordinal\", OrdinalEncoder(), categorical_cols),\n",
        "            (\"scaler\", StandardScaler(), numerical_cols)\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Create model pipeline\n",
        "    pipeline = Pipeline(\n",
        "        steps=[ \n",
        "            (\"preprocessor\", preprocessor),\n",
        "            (\"model\", XGBClassifier(random_state=42))\n",
        "        ]\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    X_train = train_data[feature_cols]\n",
        "    y_train = train_data[target_col]\n",
        "    \n",
        "    pipeline.fit(X_train, y_train)\n",
        "    \n",
        "    # Evaluate on training set\n",
        "    train_predictions = pipeline.predict(X_train)\n",
        "    train_f1 = f1_score(y_train, train_predictions)\n",
        "    \n",
        "    # Evaluate on test set\n",
        "    X_test = test_data[feature_cols]\n",
        "    y_test = test_data[target_col]\n",
        "    \n",
        "    test_predictions = pipeline.predict(X_test)\n",
        "    test_f1 = f1_score(y_test, test_predictions)\n",
        "    \n",
        "    return {\n",
        "        'model': pipeline,\n",
        "        'train_f1_score': train_f1,\n",
        "        'test_f1_score': test_f1\n",
        "    }\n",
        "\n",
        "# Train the model\n",
        "model_result = train_churn_model(training_dataset)\n",
        "\n",
        "print(f\"‚úÖ Model training completed:\")\n",
        "print(f\"   Training F1 Score: {model_result['train_f1_score']:.4f}\")\n",
        "print(f\"   Test F1 Score: {model_result['test_f1_score']:.4f}\")\n",
        "\n",
        "# Validate on validation dataset\n",
        "validation_df = validation_dataset.to_pandas()\n",
        "val_predictions = model_result['model'].predict(validation_df[feature_cols])\n",
        "val_f1 = f1_score(validation_df[target_col], val_predictions)\n",
        "\n",
        "print(f\"   Validation F1 Score: {val_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Enhanced Model Performance Functions - Using sklearn for direct F1 calculation\n",
        "\n",
        "def get_model_performance_sklearn(prediction_column, table_name='CUSTOMER_CHURN_PREDICTED_PROD2', limit_latest_timestamp=True):\n",
        "    \"\"\"\n",
        "    Get F1 score using sklearn directly from the prediction table\n",
        "    \n",
        "    Parameters:\n",
        "    - prediction_column: str, one of 'CHURNED_PRED_PROD', 'CHURNED_PRED_BASE', 'CHURNED_PRED_RETRAIN'\n",
        "    - table_name: str, table containing predictions and true labels (default: 'CUSTOMER_CHURN_PREDICTED_PROD2')\n",
        "    - limit_latest_timestamp: bool, whether to filter for latest timestamp only (default: True)\n",
        "    \n",
        "    Returns:\n",
        "    - float: F1 score calculated using sklearn\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import f1_score\n",
        "    \n",
        "    # Validate prediction column\n",
        "    valid_columns = ['CHURNED_PRED_PROD', 'CHURNED_PRED_BASE', 'CHURNED_PRED_RETRAIN']\n",
        "    if prediction_column not in valid_columns:\n",
        "        raise ValueError(f\"prediction_column must be one of {valid_columns}\")\n",
        "    \n",
        "    try:\n",
        "        # Get the data from the table\n",
        "        table_df = session.table(f'{sc}.{table_name}')\n",
        "        \n",
        "        if limit_latest_timestamp:\n",
        "            # Get latest timestamp with labels (same logic as original function)\n",
        "            timestamps = session.table(f'{sc}.{labeled_table}').select(\"TIMESTAMP\").distinct().sort(\"TIMESTAMP\").collect()\n",
        "            latest_timestamp = timestamps[-2][\"TIMESTAMP\"]\n",
        "            \n",
        "            # Filter for latest timestamp\n",
        "            table_df = table_df.filter(F.col(\"TIMESTAMP\") == latest_timestamp)\n",
        "        \n",
        "        # Get only rows where both true labels and predictions are not null\n",
        "        filtered_df = table_df.filter(\n",
        "            (F.col(\"CHURNED\").is_not_null()) & \n",
        "            (F.col(prediction_column).is_not_null())\n",
        "        ).select(\"CHURNED\", prediction_column)\n",
        "        \n",
        "        # Convert to pandas for sklearn\n",
        "        pandas_df = filtered_df.to_pandas()\n",
        "        \n",
        "        if len(pandas_df) == 0:\n",
        "            print(f\"üìçüìç Warning: No data available for {prediction_column}\")\n",
        "            return 1.0  # Default high score if no data\n",
        "        \n",
        "        # Calculate F1 score using sklearn\n",
        "        y_true = pandas_df['CHURNED'].astype(int)\n",
        "        y_pred = pandas_df[prediction_column].astype(int)\n",
        "        \n",
        "        f1 = f1_score(y_true, y_pred)\n",
        "        \n",
        "        return f1\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"üìçüìç Warning: Error calculating F1 score for {prediction_column}: {str(e)}\")\n",
        "        return 1.0  # Default if calculation fails\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Registering model in Model Registry...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ccarrero/opt/anaconda3/envs/py311_env/lib/python3.11/site-packages/snowflake/ml/model/model_signature.py:71: UserWarning: The sample input has 100 rows. Using the first 100 rows to define the inputs and outputs of the model and the data types of each. Use `signatures` parameter to specify model inputs and outputs manually if the automatic inference is not correct.\n",
            "  warnings.warn(\n",
            "/Users/ccarrero/opt/anaconda3/envs/py311_env/lib/python3.11/site-packages/snowflake/ml/model/_model_composer/model_composer.py:227: UserWarning: `relax_version` is not set and therefore defaulted to True. Dependency version constraints relaxed from ==x.y.z to >=x.y, <(x+1). To use specific dependency versions for compatibility, reproducibility, etc., set `options={'relax_version': False}` when logging the model.\n",
            "  self.manifest.save(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Baseline model registered and set as default\n",
            "   Model name: ChurnDetector\n",
            "   Version: baseline\n",
            "   F1 Score: 0.7670\n"
          ]
        }
      ],
      "source": [
        "# Register model in Model Registry\n",
        "print(\"üîÑ Registering model in Model Registry...\")\n",
        "\n",
        "# Log the trained model\n",
        "baseline_model = mr.log_model(\n",
        "    model=model_result['model'],\n",
        "    model_name=\"ChurnDetector\",\n",
        "    version_name=\"baseline\",\n",
        "    conda_dependencies=[\"snowflake-ml-python\", \"xgboost\", \"scikit-learn\"],\n",
        "    sample_input_data=training_dataset.select(feature_cols).limit(100),\n",
        "    task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n",
        "    target_platforms=[\"WAREHOUSE\", \"SNOWPARK_CONTAINER_SERVICES\"],                \n",
        "    comment=\"Baseline model for customer churn detection\"\n",
        ")\n",
        "\n",
        "# Set metrics for the model\n",
        "baseline_model.set_metric(metric_name=\"train_f1_score\", value=model_result['train_f1_score'])\n",
        "baseline_model.set_metric(metric_name=\"test_f1_score\", value=model_result['test_f1_score'])\n",
        "baseline_model.set_metric(metric_name=\"validation_f1_score\", value=val_f1)\n",
        "\n",
        "# Set as default version\n",
        "session.sql(f'USE SCHEMA {mr_schema}').collect()\n",
        "session.sql('ALTER MODEL ChurnDetector SET DEFAULT_VERSION = baseline;').collect()\n",
        "session.sql(f'USE SCHEMA {sc}').collect()\n",
        "\n",
        "print(\"‚úÖ Baseline model registered and set as default\")\n",
        "print(f\"   Model name: ChurnDetector\")\n",
        "print(f\"   Version: baseline\")\n",
        "print(f\"   F1 Score: {val_f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Iterative Model Improvement & Observability\n",
        "\n",
        "Setting up model monitoring and implementing continuous learning with new data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Setting up model monitoring infrastructure...\n",
            "‚úÖ Monitoring tables created\n",
            "‚úÖ Inference function defined\n"
          ]
        }
      ],
      "source": [
        "# Setup Model Monitoring Infrastructure\n",
        "print(\"üîÑ Setting up model monitoring infrastructure...\")\n",
        "\n",
        "# Create prediction tables for monitoring - using single table format like CC_RETAIL_4_3_STREAMS_TRAINING\n",
        "monitoring_tables = [\"customer_churn_baseline_predicted\", \"CUSTOMER_CHURN_PREDICTED_PROD2\"]\n",
        "\n",
        "for table in monitoring_tables:\n",
        "    session.sql(f\"\"\"       \n",
        "        CREATE OR REPLACE TABLE {table} (\n",
        "            CUSTOMER_ID VARCHAR(16777216),\n",
        "            TIMESTAMP TIMESTAMP_NTZ(9),\n",
        "            GENDER VARCHAR(16777216),\n",
        "            LOCATION VARCHAR(16777216),\n",
        "            CUSTOMER_SEGMENT VARCHAR(16777216),\n",
        "            LAST_PURCHASE_DATE DATE,\n",
        "            NEXT_TRANSACTION_DATE DATE,\n",
        "            AGE FLOAT,\n",
        "            SENTIMENT_MIN_2 FLOAT,\n",
        "            SENTIMENT_MIN_3 FLOAT,\n",
        "            SENTIMENT_MIN_4 FLOAT,\n",
        "            SENTIMENT_AVG_2 FLOAT,\n",
        "            SENTIMENT_AVG_3 FLOAT,\n",
        "            SENTIMENT_AVG_4 FLOAT,\n",
        "            SUM_TOTAL_AMOUNT_PAST_7D FLOAT,\n",
        "            SUM_TOTAL_AMOUNT_PAST_1MM FLOAT,\n",
        "            SUM_TOTAL_AMOUNT_PAST_2MM FLOAT,\n",
        "            SUM_TOTAL_AMOUNT_PAST_3MM FLOAT,\n",
        "            COUNT_ORDERS_PAST_7D FLOAT,\n",
        "            COUNT_ORDERS_PAST_1MM FLOAT,\n",
        "            COUNT_ORDERS_PAST_2MM FLOAT,\n",
        "            COUNT_ORDERS_PAST_3MM FLOAT,\n",
        "            DAYS_SINCE_LAST_PURCHASE FLOAT,\n",
        "            CHURNED FLOAT,\n",
        "            CHURNED_PRED_PROD FLOAT,\n",
        "            CHURNED_PRED_BASE FLOAT,\n",
        "            CHURNED_PRED_RETRAIN FLOAT,\n",
        "            CHURNED_PRED_PROBABILITY FLOAT,\n",
        "            VERSION_NAME VARCHAR(50)\n",
        "        )\n",
        "    \"\"\").collect()\n",
        "\n",
        "print(\"‚úÖ Monitoring tables created\")\n",
        "\n",
        "# Create inference function\n",
        "def run_inference(model, dataset_df, output_table, col_name, is_prod):\n",
        "    \"\"\"Run inference and store results for monitoring\"\"\"\n",
        "    \n",
        "    # Get predictions\n",
        "    predictions = model.run(dataset_df, function_name=\"predict\")\n",
        "    predictions = predictions.select([F.col(c).alias(c.replace('\"', '')) for c in predictions.columns])\n",
        "    predictions_df = predictions.rename(\"output_feature_0\", col_name)\n",
        "    predictions_df = predictions_df.with_column(\"VERSION_NAME\", F.lit(model.version_name))\n",
        "    predictions_df = predictions_df.with_column(\"CHURNED_PRED_PROBABILITY\", F.col(col_name))\n",
        "    \n",
        "    # Store in temporary table first\n",
        "    predictions_df.write.mode(\"overwrite\").save_as_table('TEMP_PREDICTIONS')\n",
        "    \n",
        "    # Merge with output table\n",
        "    output_columns = [field.name for field in session.table(output_table).schema]\n",
        "    insert_columns = \", \".join(output_columns)\n",
        "    insert_values = \", \".join([\n",
        "        f\"t.{col}\" if col in predictions_df.columns else \"NULL\" for col in output_columns\n",
        "    ])\n",
        "    \n",
        "    merge_statement = f\"\"\"\n",
        "        MERGE INTO {output_table} o\n",
        "        USING TEMP_PREDICTIONS t\n",
        "        ON o.CUSTOMER_ID = t.CUSTOMER_ID AND o.TIMESTAMP = t.TIMESTAMP\n",
        "        WHEN MATCHED THEN\n",
        "            UPDATE SET o.{col_name} = t.{col_name},\n",
        "                       o.VERSION_NAME = t.VERSION_NAME,\n",
        "                       o.CHURNED_PRED_PROBABILITY = t.CHURNED_PRED_PROBABILITY\n",
        "        WHEN NOT MATCHED THEN\n",
        "            INSERT ({insert_columns})\n",
        "            VALUES ({insert_values})\n",
        "    \"\"\"\n",
        "    \n",
        "    session.sql(merge_statement).collect()\n",
        "    print(f\"‚úÖ Predictions stored in {output_table}\")\n",
        "\n",
        "print(\"‚úÖ Inference function defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Creating Model Monitors...\n",
            "‚úÖ Predictions stored in customer_churn_baseline_predicted\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Model Monitors created\n",
            "   - Monitor_ChurnDetector_Base\n",
            "   - Monitor_ChurnDetector_Prod\n",
            "   - Monitor_ChurnDetector_Retrain\n"
          ]
        }
      ],
      "source": [
        "# Setup Model Monitors\n",
        "print(\"üîÑ Creating Model Monitors...\")\n",
        "session.sql(f'USE SCHEMA {sc}').collect()\n",
        "\n",
        "# Populate baseline predictions\n",
        "baseline_model = mr.get_model(\"ChurnDetector\").version(\"baseline\")\n",
        "run_inference(baseline_model, training_dataset, 'customer_churn_baseline_predicted', 'CHURNED_PRED_BASE', True)\n",
        "run_inference(baseline_model, validation_dataset, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_PROD', True)\n",
        "run_inference(baseline_model, validation_dataset, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_BASE', True)\n",
        "run_inference(baseline_model, validation_dataset, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_RETRAIN', True)\n",
        "\n",
        "# Need this as each model monitor requires a unique model version\n",
        "fake_prod_model = mr.get_model(\"ChurnDetector\").version(\"baseline\")\n",
        "\n",
        "fake_prod_logged = mr.log_model(model= fake_prod_model,\n",
        "                        model_name= \"ChurnDetector\",\n",
        "                        version_name= \"PRODMONITOR\",\n",
        "                        )\n",
        "\n",
        "fake_retrain_model = mr.get_model(\"ChurnDetector\").version(\"baseline\")\n",
        "\n",
        "fake_retrain_logged = mr.log_model(model= fake_retrain_model,\n",
        "                        model_name= \"ChurnDetector\",\n",
        "                        version_name= \"RETRAIN\",\n",
        "                        )\n",
        "\n",
        "\n",
        "# Create Model Monitors\n",
        "session.sql(f'USE SCHEMA {mr_schema}').collect()\n",
        "\n",
        "# Monitor for baseline model\n",
        "session.sql(f\"\"\"\n",
        "    CREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_Base\n",
        "    WITH\n",
        "        MODEL=ChurnDetector\n",
        "        VERSION=baseline\n",
        "        FUNCTION=predict\n",
        "        SOURCE={sc}.CUSTOMER_CHURN_PREDICTED_PROD2\n",
        "        BASELINE={sc}.customer_churn_baseline_predicted\n",
        "        TIMESTAMP_COLUMN=TIMESTAMP\n",
        "        PREDICTION_CLASS_COLUMNS=(CHURNED_PRED_BASE)  \n",
        "        ACTUAL_CLASS_COLUMNS=(CHURNED)\n",
        "        ID_COLUMNS=(CUSTOMER_ID)\n",
        "        WAREHOUSE={WAREHOUSE}\n",
        "        REFRESH_INTERVAL='1 min'\n",
        "        AGGREGATION_WINDOW='1 day';\n",
        "\"\"\").collect()\n",
        "\n",
        "# Monitor for production model\n",
        "session.sql(f\"\"\"\n",
        "    CREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_Prod\n",
        "    WITH\n",
        "        MODEL=ChurnDetector\n",
        "        VERSION=PRODMONITOR\n",
        "        FUNCTION=predict\n",
        "        SOURCE={sc}.CUSTOMER_CHURN_PREDICTED_PROD2\n",
        "        BASELINE={sc}.customer_churn_baseline_predicted\n",
        "        TIMESTAMP_COLUMN=TIMESTAMP\n",
        "        PREDICTION_CLASS_COLUMNS=(CHURNED_PRED_PROD)  \n",
        "        ACTUAL_CLASS_COLUMNS=(CHURNED)\n",
        "        ID_COLUMNS=(CUSTOMER_ID)\n",
        "        WAREHOUSE={WAREHOUSE}\n",
        "        REFRESH_INTERVAL='1 min'\n",
        "        AGGREGATION_WINDOW='1 day';\n",
        "\"\"\").collect()\n",
        "\n",
        "# Monitor for retrained model\n",
        "session.sql(f\"\"\"\n",
        "    CREATE OR REPLACE MODEL MONITOR Monitor_ChurnDetector_Retrain\n",
        "    WITH\n",
        "        MODEL=ChurnDetector\n",
        "        VERSION=RETRAIN\n",
        "        FUNCTION=predict\n",
        "        SOURCE={sc}.CUSTOMER_CHURN_PREDICTED_PROD2\n",
        "        BASELINE={sc}.customer_churn_baseline_predicted\n",
        "        TIMESTAMP_COLUMN=TIMESTAMP\n",
        "        PREDICTION_CLASS_COLUMNS=(CHURNED_PRED_RETRAIN)  \n",
        "        ACTUAL_CLASS_COLUMNS=(CHURNED)\n",
        "        ID_COLUMNS=(CUSTOMER_ID)\n",
        "        WAREHOUSE={WAREHOUSE}\n",
        "        REFRESH_INTERVAL='1 min'\n",
        "        AGGREGATION_WINDOW='1 day';\n",
        "\"\"\").collect()\n",
        "\n",
        "session.sql(f'USE SCHEMA {sc}').collect()\n",
        "\n",
        "print(\"‚úÖ Model Monitors created\")\n",
        "print(\"   - Monitor_ChurnDetector_Base\")\n",
        "print(\"   - Monitor_ChurnDetector_Prod\")\n",
        "print(\"   - Monitor_ChurnDetector_Retrain\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Monitoring and retraining functions defined\n"
          ]
        }
      ],
      "source": [
        "# Monitoring and Retraining Functions\n",
        "# get_model_performance() function removed - now using get_model_performance_sklearn() only\n",
        "\n",
        "def update_labels_with_new_data(db: str, sc:str, baseline_table: str,  \n",
        "                     num_days_churn: int):\n",
        "\n",
        "    # Load baseline features dataset\n",
        "    baseline_table = f'{db}.{sc}.{baseline_table}'\n",
        "    \n",
        "    baseline_df = session.table(baseline_table)\n",
        "\n",
        "    # Load sales dataset\n",
        "    sales_df = session.table(f'{db}.{sc}.SALES')\n",
        "\n",
        "    # Filter sales to retain only customer ID and transaction date\n",
        "    sales_filtered = sales_df.select(F.col(\"CUSTOMER_ID\"), F.col(\"TRANSACTION_DATE\"))\n",
        "\n",
        "    # Find the next transaction date for each (CUSTOMER_ID, TIMESTAMP)\n",
        "    next_transaction_df = (\n",
        "        baseline_df\n",
        "        .join(sales_filtered, \"CUSTOMER_ID\", \"left\")\n",
        "        .filter(F.col(\"TRANSACTION_DATE\") >F.col(\"LAST_PURCHASE_DATE\"))\n",
        "        .group_by(F.col(\"CUSTOMER_ID\"), F.col(\"TIMESTAMP\"))\n",
        "        .agg(F.min(\"TRANSACTION_DATE\").alias(\"NEXT_TX_DATE\"))\n",
        "    )\n",
        "\n",
        "    # Join back with the baseline dataset to compute CHURNED\n",
        "    final_df = (\n",
        "        baseline_df\n",
        "        .join(next_transaction_df, [\"CUSTOMER_ID\", \"TIMESTAMP\"], \"left\")\n",
        "        .select(\n",
        "            baseline_df[\"CUSTOMER_ID\"],\n",
        "            baseline_df[\"TIMESTAMP\"],\n",
        "            next_transaction_df[\"NEXT_TX_DATE\"],\n",
        "            F.when(\n",
        "                next_transaction_df[\"NEXT_TX_DATE\"].is_null() |\n",
        "                ((next_transaction_df[\"NEXT_TX_DATE\"] - baseline_df[\"LAST_PURCHASE_DATE\"]) > num_days_churn),\n",
        "                1\n",
        "            ).otherwise(0).alias(\"CHURNED\")\n",
        "        )    \n",
        "        .with_column_renamed(\"NEXT_TX_DATE\", \"NEXT_TRANSACTION_DATE\")\n",
        "\n",
        "    )\n",
        "\n",
        "    final_df.write.mode(\"overwrite\").save_as_table('temp_updates')\n",
        "\n",
        "    update_statement = f\"\"\"\n",
        "        update {baseline_table} c\n",
        "        set CHURNED = t.CHURNED,\n",
        "            NEXT_TRANSACTION_DATE = t.NEXT_TRANSACTION_DATE\n",
        "        from temp_updates t\n",
        "        where c.CUSTOMER_ID = t.CUSTOMER_ID AND\n",
        "            c.TIMESTAMP = t.TIMESTAMP\n",
        "          \n",
        "        \"\"\"\n",
        "\n",
        "    session.sql(update_statement).collect()\n",
        "    \n",
        "    print(\"‚úÖ Labels updated with new transaction data\")\n",
        "\n",
        "def set_default_model(version_name):\n",
        "    \"\"\"Set default model version\"\"\"\n",
        "    session.sql(f'USE SCHEMA {mr_schema}').collect()\n",
        "    session.sql(f'ALTER MODEL ChurnDetector SET DEFAULT_VERSION = {version_name};').collect()\n",
        "    session.sql(f'USE SCHEMA {sc}').collect()\n",
        "    print(f\"‚úÖ Default model set to: {version_name}\")\n",
        "\n",
        "print(\"‚úÖ Monitoring and retraining functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "from snowflake.ml import dataset\n",
        "\n",
        "def get_inference_dataset():\n",
        "\n",
        "    ts_inference_tb = session.table(labeled_table).select(\"TIMESTAMP\").distinct().sort(\"TIMESTAMP\").collect()\n",
        "    \n",
        "    ts_inference = ts_inference_tb[-2][\"TIMESTAMP\"]\n",
        "\n",
        "    date_name = \"v_\" + str(ts_inference).replace(\"-\", \"_\")\n",
        "    ds_name = f'{fs_schema}.CHURN_{date_name}'\n",
        "\n",
        "    inference_dataset = dataset.load_dataset(session, ds_name, 'v1')\n",
        "\n",
        "    inference_dataset_sdf = inference_dataset.read.to_snowpark_dataframe()\n",
        "\n",
        "    return inference_dataset_sdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Finished!\n"
          ]
        }
      ],
      "source": [
        "# Main ML Pipeline Function\n",
        "# UPDATED process_monthly_data() function using sklearn method only\n",
        "# Removed get_model_performance() and 90-second waits\n",
        "\n",
        "def process_monthly_data():\n",
        "    \"\"\"Process new monthly data and retrain if needed - NOW USING SKLEARN METHOD ONLY\"\"\"\n",
        "    start_time = datetime.now()\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"üîÑ Processing new monthly data...\")\n",
        "    \n",
        "    # Step 1: Load new data\n",
        "    if not copy_next_file(session, db, sc):\n",
        "        print(\"‚ùå No more data files to process\")\n",
        "        return False\n",
        "    \n",
        "    # Step 2: Process sentiment\n",
        "    process_sentiment()\n",
        "    \n",
        "    # Step 3: Create features for new data\n",
        "    sales_df = session.table(\"SALES\")\n",
        "    latest_timestamp = sales_df.select(F.max(\"TRANSACTION_DATE\")).collect()[0][0]\n",
        "    \n",
        "    print(f\"üìä Creating features for timestamp: {latest_timestamp}\")\n",
        "    create_customer_features(session, db, sc, latest_timestamp, features_table)\n",
        "    \n",
        "    # Step 4: Update labels with new data\n",
        "    create_churn_labels(session, db, sc, features_table, labeled_table, CHURN_WINDOW)\n",
        "    \n",
        "    latest_labeled_timestamp = session.table(labeled_table).select(F.max(\"TIMESTAMP\")).collect()[0][0]\n",
        "\n",
        "    # Step 5: Create new dataset\n",
        "    date_name = f\"v_{latest_labeled_timestamp}\".replace(\"-\", \"_\")\n",
        "    new_dataset = create_dataset(fs, feature_view, f'CHURN_{date_name}', latest_labeled_timestamp)\n",
        "    \n",
        "    # Step 6: Run inference with baseline model\n",
        "    baseline_model = mr.get_model(\"ChurnDetector\").version(\"baseline\")\n",
        "    run_inference(baseline_model, new_dataset, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_BASE', False)\n",
        "    update_labels_with_new_data(db, sc, \"CUSTOMER_CHURN_PREDICTED_PROD2\", CHURN_WINDOW)\n",
        "\n",
        "    # No longer need to wait for monitors since we use sklearn directly\n",
        "    print(\"üìà Model Performance Metrics:\")\n",
        "    \n",
        "    # Get performance using sklearn method only (fast and reliable)\n",
        "    baseline_f1 = get_model_performance_sklearn('CHURNED_PRED_BASE')\n",
        "    production_f1 = get_model_performance_sklearn('CHURNED_PRED_PROD')\n",
        "    \n",
        "    print(f\"   Baseline F1:   {baseline_f1:.4f}\")\n",
        "    print(f\"   Production F1: {production_f1:.4f}\")\n",
        "    \n",
        "    # Step 8: Retrain if performance drops\n",
        "    retrain_threshold = 0.8\n",
        "    retrained_f1 = 0.0\n",
        "    \n",
        "    if production_f1 < retrain_threshold:\n",
        "        print(f\"üö® Performance dropped below {retrain_threshold}, retraining model...\")\n",
        "        \n",
        "        # Train new model\n",
        "        new_model_result = train_churn_model(new_dataset)\n",
        "        \n",
        "        # Register new model\n",
        "        retrained_model = mr.log_model(\n",
        "            model=new_model_result['model'],\n",
        "            model_name=\"ChurnDetector\",\n",
        "            version_name=date_name,\n",
        "            conda_dependencies=[\"snowflake-ml-python\", \"xgboost\", \"scikit-learn\"],\n",
        "            sample_input_data=new_dataset.select(feature_cols).limit(100),\n",
        "            task=type_hints.Task.TABULAR_BINARY_CLASSIFICATION,\n",
        "            comment=f\"Retrained model for {latest_labeled_timestamp}\"\n",
        "        )\n",
        "        \n",
        "        retrained_model.set_metric(\"train_f1_score\", new_model_result['train_f1_score'])\n",
        "        retrained_model.set_metric(\"test_f1_score\", new_model_result['test_f1_score'])\n",
        "        \n",
        "        print(f\"‚úÖ New model {date_name} trained:\")\n",
        "        print(f\"   Train F1: {new_model_result['train_f1_score']:.4f}\")\n",
        "        print(f\"   Test F1: {new_model_result['test_f1_score']:.4f}\")\n",
        "        \n",
        "        # Test new model on validation data\n",
        "        val_dataset = get_inference_dataset()\n",
        "        run_inference(retrained_model, val_dataset, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_RETRAIN', True)\n",
        "            \n",
        "        # Get retrained model performance using sklearn method\n",
        "        retrained_f1 = get_model_performance_sklearn('CHURNED_PRED_RETRAIN')\n",
        "        print(f\"   Validation F1: {retrained_f1:.4f}\")\n",
        "\n",
        "    # Step 9: Choose best model\n",
        "\n",
        "    best_model = mr.get_model(\"ChurnDetector\").default.version_name\n",
        "    \n",
        "    if (baseline_f1 > production_f1) & (baseline_f1 > retrained_f1):\n",
        "        set_default_model('baseline')\n",
        "        best_model = \"BASELINE\"\n",
        "    elif (retrained_f1 > baseline_f1) & (retrained_f1 > production_f1):\n",
        "        set_default_model(date_name)\n",
        "        best_model = date_name\n",
        "\n",
        "    # Run production inference\n",
        "    prod_model = mr.get_model(\"ChurnDetector\").default\n",
        "    run_inference(prod_model, new_dataset, 'CUSTOMER_CHURN_PREDICTED_PROD2', 'CHURNED_PRED_PROD', True)\n",
        "    \n",
        "    elapsed = datetime.now() - start_time\n",
        "    print(f\"‚úÖ Monthly processing completed in {elapsed.total_seconds():.1f}s\")\n",
        "    print(f\"   Best model: {best_model})\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    return True\n",
        "\n",
        "print(\"‚úÖ Finished!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting continuous learning pipeline...\n",
            "This will process remaining data files and retrain models as needed\n",
            "\n",
            "\n",
            "üîÑ Processing Iteration 1\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2024-10\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "üìä Creating features for timestamp: 2024-10-31\n",
            "‚úÖ Features created for 2024-10-31\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "üìçüìç Warning: No data available for CHURNED_PRED_PROD\n",
            "   Baseline F1:   0.7627\n",
            "   Production F1: 1.0000\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 90.8s\n",
            "   Best model: BASELINE)\n",
            "============================================================\n",
            "\n",
            "üîÑ Processing Iteration 2\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2024-11\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "üìä Creating features for timestamp: 2024-11-30\n",
            "‚úÖ Features created for 2024-11-30\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "   Baseline F1:   0.7441\n",
            "   Production F1: 0.7441\n",
            "üö® Performance dropped below 0.8, retraining model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ccarrero/opt/anaconda3/envs/py311_env/lib/python3.11/site-packages/snowflake/ml/model/model_signature.py:71: UserWarning: The sample input has 100 rows. Using the first 100 rows to define the inputs and outputs of the model and the data types of each. Use `signatures` parameter to specify model inputs and outputs manually if the automatic inference is not correct.\n",
            "  warnings.warn(\n",
            "/Users/ccarrero/opt/anaconda3/envs/py311_env/lib/python3.11/site-packages/snowflake/ml/model/_model_composer/model_composer.py:227: UserWarning: `relax_version` is not set and therefore defaulted to True. Dependency version constraints relaxed from ==x.y.z to >=x.y, <(x+1). To use specific dependency versions for compatibility, reproducibility, etc., set `options={'relax_version': False}` when logging the model.\n",
            "  self.manifest.save(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ New model v_2024_11_30 trained:\n",
            "   Train F1: 1.0000\n",
            "   Test F1: 0.9641\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "   Validation F1: 0.7101\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 140.0s\n",
            "   Best model: BASELINE)\n",
            "============================================================\n",
            "\n",
            "üîÑ Processing Iteration 3\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2024-12\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "üìä Creating features for timestamp: 2024-12-20\n",
            "‚úÖ Features created for 2024-12-20\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "   Baseline F1:   0.6876\n",
            "   Production F1: 0.6876\n",
            "üö® Performance dropped below 0.8, retraining model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/ccarrero/opt/anaconda3/envs/py311_env/lib/python3.11/site-packages/snowflake/ml/model/model_signature.py:71: UserWarning: The sample input has 100 rows. Using the first 100 rows to define the inputs and outputs of the model and the data types of each. Use `signatures` parameter to specify model inputs and outputs manually if the automatic inference is not correct.\n",
            "  warnings.warn(\n",
            "/Users/ccarrero/opt/anaconda3/envs/py311_env/lib/python3.11/site-packages/snowflake/ml/model/_model_composer/model_composer.py:227: UserWarning: `relax_version` is not set and therefore defaulted to True. Dependency version constraints relaxed from ==x.y.z to >=x.y, <(x+1). To use specific dependency versions for compatibility, reproducibility, etc., set `options={'relax_version': False}` when logging the model.\n",
            "  self.manifest.save(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ New model v_2024_12_20 trained:\n",
            "   Train F1: 1.0000\n",
            "   Test F1: 0.9978\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "   Validation F1: 0.7771\n",
            "‚úÖ Default model set to: v_2024_12_20\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 147.0s\n",
            "   Best model: v_2024_12_20)\n",
            "============================================================\n",
            "\n",
            "üîÑ Processing Iteration 4\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "No unprocessed sales files found.\n",
            "‚ùå No more data files to process\n",
            "\n",
            "üéØ All data processed!\n",
            "\n",
            "‚úÖ Pipeline completed after 3 iterations\n",
            "\n",
            "============================================================\n",
            "üìä FINAL SUMMARY\n",
            "============================================================\n",
            "üìà Data Processed:\n",
            "   Sales records: 54,837\n",
            "   Feedback records: 6,685\n",
            "   Feature records: 34,040\n",
            "\n",
            "ü§ñ Models in Registry:\n",
            "   Model information not available\n",
            "\n",
            "üéØ Workflow completed successfully!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Run the continuous learning pipeline\n",
        "print(\"üöÄ Starting continuous learning pipeline...\")\n",
        "print(\"This will process remaining data files and retrain models as needed\\n\")\n",
        "\n",
        "# Track processing iterations\n",
        "iteration = 1\n",
        "continue_processing = True\n",
        "\n",
        "while continue_processing:\n",
        "    print(f\"\\nüîÑ Processing Iteration {iteration}\")\n",
        "    \n",
        "    #try:\n",
        "    continue_processing = process_monthly_data()\n",
        "    if continue_processing:\n",
        "        iteration += 1\n",
        "    else:\n",
        "        print(\"\\nüéØ All data processed!\")\n",
        "            \n",
        "#    except Exception as e:\n",
        "#        print(f\"‚ùå Error in iteration {iteration}: {str(e)}\")\n",
        "        #break\n",
        "\n",
        "print(f\"\\n‚úÖ Pipeline completed after {iteration-1} iterations\")\n",
        "\n",
        "# Final summary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä FINAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check final data counts\n",
        "sales_final = session.table(\"SALES\").count()\n",
        "feedback_final = session.table(\"FEEDBACK_SENTIMENT\").count()\n",
        "features_final = session.table(labeled_table).count()\n",
        "\n",
        "print(f\"üìà Data Processed:\")\n",
        "print(f\"   Sales records: {sales_final:,}\")\n",
        "print(f\"   Feedback records: {feedback_final:,}\")\n",
        "print(f\"   Feature records: {features_final:,}\")\n",
        "\n",
        "# Check model registry\n",
        "models_query = session.sql(f\"USE SCHEMA {mr_schema}; SHOW MODELS; USE SCHEMA {sc};\")\n",
        "print(f\"\\nü§ñ Models in Registry:\")\n",
        "try:\n",
        "    models = session.sql(f\"\"\"\n",
        "        SELECT VERSION_NAME, CREATION_TIME \n",
        "        FROM {mr_schema}.INFORMATION_SCHEMA.ML_MODELS \n",
        "        WHERE MODEL_NAME = 'CHURNDETECTOR'\n",
        "        ORDER BY CREATION_TIME\n",
        "    \"\"\").collect()\n",
        "    \n",
        "    for model in models:\n",
        "        print(f\"   - {model['VERSION_NAME']} (created: {model['CREATION_TIME']})\")\n",
        "except:\n",
        "    print(\"   Model information not available\")\n",
        "\n",
        "print(f\"\\nüéØ Workflow completed successfully!\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'V_2024_12_20'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "my_model = mr.get_model(\"ChurnDetector\").default.version_name\n",
        "\n",
        "my_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Introducing data drift with new customer segments...\n",
            "üì• Loading new customers with different demographics...\n",
            "‚úÖ New customers loaded - this introduces demographic drift\n",
            "üîç Discovering new data files for drift simulation...\n",
            "üìä Registered 5 new sales files and 13 new feedback files\n",
            "   These files contain:\n",
            "   - Different customer segments (demographic drift)\n",
            "   - New purchasing patterns (behavioral drift)\n",
            "   - Different sentiment distributions (feature drift)\n",
            "\n",
            "üìã Unprocessed files available:\n",
            "   - feedback_raw: 18 files\n",
            "   - sales: 5 files\n",
            "\n",
            "‚úÖ Data drift setup completed - ready to process new data patterns\n"
          ]
        }
      ],
      "source": [
        "# Introduce Data Drift - New Customer Segments and Behaviors\n",
        "print(\"üîÑ Introducing data drift with new customer segments...\")\n",
        "\n",
        "# Load new customers with different demographic patterns\n",
        "print(\"üì• Loading new customers with different demographics...\")\n",
        "load_into_table(session, f'{db}.{sc}.CUSTOMERS', f'@{stage_name}/new_customers.csv')\n",
        "\n",
        "print(\"‚úÖ New customers loaded - this introduces demographic drift\")\n",
        "\n",
        "# Discover and register new drift data files\n",
        "print(\"üîç Discovering new data files for drift simulation...\")\n",
        "\n",
        "# Get new sales files (with different purchasing patterns)\n",
        "new_sales_files = get_year_month_files(session, stage_name, 'new_sales')\n",
        "insert_file_tracking('sales', db, sc, new_sales_files)\n",
        "\n",
        "# Get new feedback files (with different sentiment patterns)  \n",
        "new_feedback_files = get_year_month_files(session, stage_name, 'new_feedback_raw2')\n",
        "insert_file_tracking('feedback_raw', db, sc, new_feedback_files)\n",
        "\n",
        "print(f\"üìä Registered {len(new_sales_files)} new sales files and {len(new_feedback_files)} new feedback files\")\n",
        "print(\"   These files contain:\")\n",
        "print(\"   - Different customer segments (demographic drift)\")\n",
        "print(\"   - New purchasing patterns (behavioral drift)\")  \n",
        "print(\"   - Different sentiment distributions (feature drift)\")\n",
        "\n",
        "# Check what files are available for processing\n",
        "unprocessed_files = session.sql(f\"\"\"\n",
        "    SELECT FILE_TYPE, COUNT(*) as COUNT\n",
        "    FROM FILES_INGESTED \n",
        "    WHERE INGESTED = FALSE\n",
        "    GROUP BY FILE_TYPE\n",
        "    ORDER BY FILE_TYPE\n",
        "\"\"\").collect()\n",
        "\n",
        "print(\"\\nüìã Unprocessed files available:\")\n",
        "for row in unprocessed_files:\n",
        "    print(f\"   - {row['FILE_TYPE']}: {row['COUNT']} files\")\n",
        "\n",
        "print(\"\\n‚úÖ Data drift setup completed - ready to process new data patterns\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Processing data with drift patterns and monitoring model response...\n",
            "This will test how well our models adapt to new data distributions\n",
            "\n",
            "\n",
            "üîÑ Processing New Drift Data - Iteration 1\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2025-01\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "üìä Creating features for timestamp: 2025-01-31\n",
            "‚úÖ Features created for 2025-01-31\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "   Baseline F1:   0.5579\n",
            "   Production F1: 0.9996\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 88.9s\n",
            "   Best model: V_2024_12_20)\n",
            "============================================================\n",
            "‚úÖ Drift iteration 1 completed\n",
            "\n",
            "üîÑ Processing New Drift Data - Iteration 2\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2025-02\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "üìä Creating features for timestamp: 2025-02-28\n",
            "‚úÖ Features created for 2025-02-28\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "   Baseline F1:   0.7216\n",
            "   Production F1: 0.8948\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 82.6s\n",
            "   Best model: V_2024_12_20)\n",
            "============================================================\n",
            "‚úÖ Drift iteration 2 completed\n",
            "\n",
            "üîÑ Processing New Drift Data - Iteration 3\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2025-03\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "üìä Creating features for timestamp: 2025-03-31\n",
            "‚úÖ Features created for 2025-03-31\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "   Baseline F1:   0.7162\n",
            "   Production F1: 0.9356\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 84.5s\n",
            "   Best model: V_2024_12_20)\n",
            "============================================================\n",
            "‚úÖ Drift iteration 3 completed\n",
            "\n",
            "üîÑ Processing New Drift Data - Iteration 4\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2025-04\n",
            "‚úÖ Sentiment processed for new feedback\n",
            "üìä Creating features for timestamp: 2025-04-30\n",
            "‚úÖ Features created for 2025-04-30\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "   Baseline F1:   0.7228\n",
            "   Production F1: 0.9411\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 90.5s\n",
            "   Best model: V_2024_12_20)\n",
            "============================================================\n",
            "‚úÖ Drift iteration 4 completed\n",
            "\n",
            "üîÑ Processing New Drift Data - Iteration 5\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "üì• Loaded data for 2025-05\n",
            "üìä Creating features for timestamp: 2025-05-20\n",
            "‚úÖ Features created for 2025-05-20\n",
            "‚úÖ Labels created for churn window: 30 days\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n",
            "WARNING:root:A Column with DATE or TIMESTAMP data type detected. It might not be able to get converted to tensors. Please consider handle it in feature engineering.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Labels updated with new transaction data\n",
            "üìà Model Performance Metrics:\n",
            "   Baseline F1:   0.7128\n",
            "   Production F1: 0.9504\n",
            "‚úÖ Predictions stored in CUSTOMER_CHURN_PREDICTED_PROD2\n",
            "‚úÖ Monthly processing completed in 82.9s\n",
            "   Best model: V_2024_12_20)\n",
            "============================================================\n",
            "‚úÖ Drift iteration 5 completed\n",
            "\n",
            "üîÑ Processing New Drift Data - Iteration 6\n",
            "============================================================\n",
            "üîÑ Processing new monthly data...\n",
            "No unprocessed sales files found.\n",
            "‚ùå No more data files to process\n",
            "\n",
            "üéØ All New drift data processed!\n",
            "\n",
            "‚úÖ Data drift processing completed after 5 iterations\n"
          ]
        }
      ],
      "source": [
        "# Process Data Drift and Monitor Model Performance\n",
        "print(\"üöÄ Processing data with drift patterns and monitoring model response...\")\n",
        "print(\"This will test how well our models adapt to new data distributions\\n\")\n",
        "\n",
        "# Track drift processing\n",
        "drift_iteration = 1\n",
        "continue_drift_processing = True\n",
        "\n",
        "# Store baseline performance for comparison\n",
        "baseline_performance = {}\n",
        "\n",
        "while continue_drift_processing:\n",
        "    print(f\"\\nüîÑ Processing New Drift Data - Iteration {drift_iteration}\")\n",
        "    \n",
        "    # Process the next batch of drift data\n",
        "    continue_drift_processing = process_monthly_data()\n",
        "    \n",
        "    if continue_drift_processing:\n",
        "        print(f\"‚úÖ Drift iteration {drift_iteration} completed\")\n",
        "        \n",
        "        drift_iteration += 1\n",
        "    else:\n",
        "        print(\"\\nüéØ All New drift data processed!\")\n",
        "\n",
        "print(f\"\\n‚úÖ Data drift processing completed after {drift_iteration-1} iterations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Row(TS=datetime.datetime(2024, 7, 31, 0, 0), VERSION_NAME='BASELINE'),\n",
              " Row(TS=datetime.datetime(2024, 9, 30, 0, 0), VERSION_NAME='BASELINE'),\n",
              " Row(TS=datetime.datetime(2024, 10, 31, 0, 0), VERSION_NAME='V_2024_11_30'),\n",
              " Row(TS=datetime.datetime(2024, 11, 30, 0, 0), VERSION_NAME='V_2024_12_20'),\n",
              " Row(TS=datetime.datetime(2024, 12, 20, 0, 0), VERSION_NAME='V_2024_12_20'),\n",
              " Row(TS=datetime.datetime(2025, 1, 31, 0, 0), VERSION_NAME='V_2024_12_20'),\n",
              " Row(TS=datetime.datetime(2025, 2, 28, 0, 0), VERSION_NAME='V_2024_12_20'),\n",
              " Row(TS=datetime.datetime(2025, 3, 31, 0, 0), VERSION_NAME='V_2024_12_20'),\n",
              " Row(TS=datetime.datetime(2025, 4, 30, 0, 0), VERSION_NAME='V_2024_12_20'),\n",
              " Row(TS=datetime.datetime(2025, 5, 20, 0, 0), VERSION_NAME='V_2024_12_20')]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "session.sql('''\n",
        "select DISTINCT(TIMESTAMP) as TS, VERSION_NAME from CUSTOMER_CHURN_PREDICTED_PROD2\n",
        "order by TS ASC;''').collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a complete end-to-end ML workflow with observability\n",
        "\n",
        "### ‚úÖ What We Accomplished\n",
        "\n",
        "1. **Environment Setup**\n",
        "   - Configured Snowflake ML environment with Feature Store and Model Registry\n",
        "   - Created data schemas and staging areas\n",
        "   - Set up monitoring infrastructure\n",
        "\n",
        "2. **Data Ingestion & Processing**\n",
        "   - Loaded customer, sales, and feedback data progressively\n",
        "   - Used Cortex AI for sentiment analysis of customer feedback\n",
        "   - Created rich behavioral features using Snowflake analytical functions\n",
        "\n",
        "3. **Model Training & Registry**\n",
        "   - Trained XGBoost model for churn prediction\n",
        "   - Registered model in Model Registry with full lineage\n",
        "   - Created baseline for future comparisons\n",
        "\n",
        "4. **Continuous Learning & Monitoring**\n",
        "   - Implemented automated data ingestion pipeline\n",
        "   - Set up Model Monitors for performance tracking\n",
        "   - Created automated retraining based on performance thresholds\n",
        "   - Maintained model lineage and observability\n",
        "\n",
        "\n",
        "\n",
        "### üîç Key Features Demonstrated\n",
        "\n",
        "- **Feature Store**: Centralized feature management with versioning\n",
        "- **Model Registry**: Version control and lifecycle management for models\n",
        "- **Model Monitoring**: Automatic drift detection and performance tracking\n",
        "- **Automated Retraining**: Performance-based model updates triggered by drift\n",
        "- **Performance Evolution Tracking**: Monitor how models respond to changing data\n",
        "- **Full Observability**: Complete lineage from data to predictions with drift visibility\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "py311_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
